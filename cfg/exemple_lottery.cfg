[Hyperparams]
lr = 0.2
batch_size = 128
max_seq_len = 400
no_cuda = False
seed = 1234
num_iterations = 2000
momentum = 0.5
scheduler_steps = [1000, 1500, 1750]
scheduler_lambda = 0.5
multi_gpu = False

[Dataset]
train = path/to/kaldi/train/data/
eval = path/to/kaldi/test/data/
eval_trials = path/to/trials/file1
test = path/to/kaldi/test/data/
    path/to/kaldi/enroll/data
test_trials = path/to/trials/file1
    path/to/trials/file2
    path/to/trials/file3
features_per_frame = 30

[Model]
emb_size = 256
layers = [3, 4, 6, 3]
num_filters = [32, 64, 128, 256]
zero_init_residual = True
# [min, max, mean, std, statistical]
pooling = statistical

[Lottery]
# [lt (Lottery Ticket Hypothesis), rd (Random reinitialization)]
prune_type = lt
prune_percent = 10
prune_iterations = 35

[Outputs]
model_dir = exp/example_exp_speaker
checkpoint_interval = 10
checkpoints_dir = checkpoints
log_interval = 1